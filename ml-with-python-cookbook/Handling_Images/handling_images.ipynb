{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aeb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torchvision.models as models\n",
    "import json\n",
    "from torchvision.models import resnet18\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Images\n",
    "image = cv2.imread(\"../21568.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "type(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Images\n",
    "cv2.imwrite('new_image.jpg', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd09060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing Images\n",
    "image_100x100 = cv2.resize(image, (100,100))\n",
    "plt.imshow(image_100x100, cmap='gray'), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping Images\n",
    "image_cropped = image[:,:512]\n",
    "plt.imshow(image_cropped, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blurring Images\n",
    "image_blurry = cv2.blur(image, (100,100))\n",
    "plt.imshow(image_blurry, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b904ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Kernels\n",
    "kernel = np.ones((5,5)) / 25.0\n",
    "kernel\n",
    "image_kernel = cv2.filter2D(image, -1, kernel)\n",
    "plt.imshow(image_kernel, cmap=\"gray\"), plt.axis(\"off\"), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpening Images\n",
    "kernel = np.array([[0, -1, 0],[-1, 5,-1],[0, -1, 0]])\n",
    "image_sharp = cv2.filter2D(image, -1, kernel)\n",
    "plt.imshow(image_sharp, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhancing Contrast\n",
    "image_enhanced = cv2.equalizeHist(image)\n",
    "plt.imshow(image_enhanced, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Colored image\n",
    "image_bgr = cv2.imread(\"../21568.jpg\")\n",
    "image_yuv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YUV)\n",
    "image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0])\n",
    "image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB)\n",
    "# Show image\n",
    "plt.imshow(image_rgb), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd489ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating Colors of an image\n",
    "image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "lower_blue = np.array([50,100,50])\n",
    "upper_blue = np.array([130,255,255])\n",
    "mask = cv2.inRange(image_hsv, lower_blue, upper_blue)\n",
    "image_bgr_masked = cv2.bitwise_and(image_bgr, image_bgr, mask=mask)\n",
    "image_rgb = cv2.cvtColor(image_bgr_masked, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image_rgb), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b48586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Backgrounds\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "rectangle = (0, 56, 256, 150)\n",
    "mask = np.zeros(image_rgb.shape[:2], np.uint8)\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "cv2.grabCut(image_rgb, # Our image\n",
    "mask, # The Mask\n",
    "rectangle, # Our rectangle\n",
    "bgdModel, # Temporary array for background\n",
    "fgdModel, # Temporary array for background\n",
    "5, # Number of iterations\n",
    "cv2.GC_INIT_WITH_RECT) # Initiative using our rectangle\n",
    "# Create mask where sure and likely backgrounds set to 0, otherwise 1\n",
    "mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')\n",
    "image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]\n",
    "# Show image\n",
    "plt.imshow(image_rgb_nobg), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b05f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting edges\n",
    "median_intensity = np.median(image)\n",
    "# Set thresholds to be one standard deviation above and below median intensity\n",
    "lower_threshold = int(max(0, (1.0 - 0.33) * median_intensity))\n",
    "upper_threshold = int(min(255, (1.0 + 0.33) * median_intensity))\n",
    "image_canny = cv2.Canny(image, lower_threshold, upper_threshold)\n",
    "# Show image\n",
    "plt.imshow(image_canny, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting corners\n",
    "image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "image_gray = np.float32(image_gray)\n",
    "# Set corner detector parameters\n",
    "block_size = 2\n",
    "aperture = 29\n",
    "free_parameter = 0.04\n",
    "# Detect corners\n",
    "detector_responses = cv2.cornerHarris(image_gray,block_size,aperture,free_parameter)\n",
    "# Large corner markers\n",
    "detector_responses = cv2.dilate(detector_responses, None)\n",
    "# Only keep detector responses greater than threshold, mark as white\n",
    "threshold = 0.02\n",
    "image_bgr[detector_responses >\n",
    "threshold *\n",
    "detector_responses.max()] = [255,255,255]\n",
    "# Convert to grayscale\n",
    "image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
    "# Show image\n",
    "plt.imshow(image_gray, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features for machine learning\n",
    "image_100x100.flatten()\n",
    "plt.imshow(image_100x100, cmap=\"gray\"), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Color Histograms as Features\n",
    "features = []\n",
    "colors = ('r', 'g', 'b')\n",
    "for i, channel in enumerate(colors):\n",
    "    histogram = cv2.calcHist([image_rgb],[i], None, [256], [0,256])\n",
    "    features.extend(histogram)\n",
    "    plt.plot(histogram, color = channel)\n",
    "    plt.xlim([0,256])\n",
    "plt.show()\n",
    "observation = np.array(features).flatten()\n",
    "observation[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pretrained Embeddings as Features\n",
    "convert_tensor = transforms.ToTensor()\n",
    "pytorch_image = convert_tensor(np.array(image_rgb))\n",
    "# Load the pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "# Select the specific layer of the model we want output from\n",
    "layer = model._modules.get('avgpool')\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "# Infer the embedding with the no_grad option\n",
    "with torch.no_grad():\n",
    "    embedding = model(pytorch_image.unsqueeze(0))\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5460f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_image = tf.image.convert_image_dtype([image_rgb], tf.float32)\n",
    "# Create the model and get embeddings using the inception V1 model\n",
    "embedding_model = hub.KerasLayer(\n",
    "\"https://tfhub.dev/google/imagenet/inception_v1/feature_vector/5\"\n",
    ")\n",
    "embeddings = embedding_model(tf_image)\n",
    "# Print the shape of the embedding\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f270a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Objects with Opencv\n",
    "face_cascade = cv2.CascadeClassifier()\n",
    "face_cascade.load(\n",
    "    cv2.samples.findFile(\"models/haarcascade_frontalface_default.xml\")\n",
    ")\n",
    "image_bgr = cv2.imread(\"images/kyle_pic.jpg\", cv2.IMREAD_COLOR)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "faces = face_cascade.detectMultiScale(image_rgb)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(image_rgb, (x, y),(x + h, y + w),(0, 255, 0), 5)\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.imshow(image_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6aaf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying Images with Pytorch\n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/raghakot/keras-vis/master/resources/\"):\n",
    "    imagenet_class_index = json.load(url)\n",
    "model = resnet18(pretrained=True)\n",
    "convert_tensor = transforms.ToTensor()\n",
    "pytorch_image = convert_tensor(np.array(image_rgb))\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "prediction = model(pytorch_image.unsqueeze(0))\n",
    "# Get the index of the highest predicted probability\n",
    "_, index = torch.max(prediction, 1)\n",
    "# Convert that to a percentage value\n",
    "percentage = torch.nn.functional.softmax(prediction, dim=1)[0] * 100\n",
    "# Print the name of the item at the index along with the percent confidence\n",
    "print(imagenet_class_index[str(index.tolist()[0])][1],\n",
    "percentage[index.tolist()[0]].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
