{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925e2605",
   "metadata": {},
   "source": [
    "Unstructured text data, like the contents of a book or a tweet, is both one of the most interesting sources of features and one of the most complex to handle. \n",
    "In this chapter, we will cover strategies for transforming text into information-rich features and use some out-of-the-box features (termed embeddings) that have become increasingly ubiquitous in tasks that involve natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38303c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776b1813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Text\n",
    "text_data = [\n",
    "    \"     Interrobang. By Aishwarya Henriette \",\n",
    "    \"Parking And Going. By Karl Gautier\",\n",
    "    \"Today Is The night. By Jarek Prakash            \"\n",
    "]\n",
    "strip_whitespace = [s.strip() for s in text_data]\n",
    "strip_whitespace\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "remove_periods\n",
    "def capitalizer(s: str) -> str:\n",
    "    return s.upper()\n",
    "\n",
    "[capitalizer(s) for s in remove_periods]\n",
    "\n",
    "def replace_letters(s: str) -> str:\n",
    "    return re.sub(r'[a-zA-Z]', 'X', s)\n",
    "\n",
    "[replace_letters(s) for s in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73066b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and Cleaning HTML\n",
    "html = \"<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\"\n",
    "soup = BeautifulSoup(html)\n",
    "soup.find(\"div\", {\"class\": \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations\n",
    "text_data = ['Hi!!!! I. Love. This. Song....','10000% Agree!!!! #LoveIT','Right?!?!']\n",
    "punctuation = dict.fromkeys(\n",
    "    (i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')), None\n",
    ")\n",
    "\n",
    "[s.translate(punctuation) for s in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b444cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing Text\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "word_tokenize(string)\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing StopWords\n",
    "tokenized_words = ['i','am','going','to','go','to','the','store','and','park']\n",
    "stop_words = stopwords.words('english')\n",
    "[w for w in tokenized_words if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Words\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "porter = PorterStemmer()\n",
    "[porter.stem(w) for w in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0280311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging parts of a speech\n",
    "text_tagged = pos_tag(word_tokenize(string))\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Words\n",
    "[w for w, t in text_tagged if t in ['NN', 'NNS', 'NNP', 'NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\"I am eating a burrito for breakfast\",\"Political science is an amazing field\",\"San Francisco is an awesome city\"]\n",
    "\n",
    "tagged_tweets = []\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing named-entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n",
    "print(doc.ents)\n",
    "for e in doc.ents:\n",
    "    print(e.text, e.label_, sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f15ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding text as a bag of words\n",
    "text_data = np.array(['I love Brazil. Brazil!','Sweden is best', 'Germany beats both'])\n",
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "# Show feature matrix\n",
    "bag_of_words.toarray()\n",
    "\n",
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27a1efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "stop_words=\"english\",\n",
    "vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "# View feature matrix\n",
    "bag.toarray()\n",
    "\n",
    "# View the 1-grams and 2-grams\n",
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c999d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weighting word importance\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix.toarray()\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecc920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(np.str_('I love Brazil. Brazil!'), np.float64(0.8944271909999159)), (np.str_('Germany beats both'), np.float64(0.0)), (np.str_('Sweden is best'), np.float64(0.0))]\n"
     ]
    }
   ],
   "source": [
    "# Using Text Vectors to Calculate Text Similarity in a Search Query\n",
    "query = 'Brazil'\n",
    "vector = tfidf.transform([query])\n",
    "cosine_similarities = linear_kernel(vector, feature_matrix).flatten()\n",
    "related_doc_indicies = cosine_similarities.argsort()[:-10:-1]\n",
    "print([(text_data[i], cosine_similarities[i]) for i in related_doc_indicies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis Classifier\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "sentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\n",
    "sentiment_2 = classifier(\"Machine learning is the absolute bees knees I love it so much!\")\n",
    "print(sentiment_1, sentiment_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
